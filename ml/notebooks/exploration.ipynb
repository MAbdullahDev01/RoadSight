{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54df0cf9-ea96-458e-8de0-b0fbd963320f",
   "metadata": {},
   "source": [
    "# RoadSight — Dataset Exploration\n",
    "\n",
    "This notebook is used to:\n",
    "- Understand the RDD2020 dataset\n",
    "- Inspect images and labels\n",
    "- Learn how ML data is structured\n",
    "\n",
    "No training happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a965b0f-604b-4bea-9191-417912adf362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.14.0 (tags/v3.14.0:ebf955d, Oct  7 2025, 10:15:03) [MSC v.1944 64 bit (AMD64)]\n",
      "Current directory: c:\\Users\\saima\\OneDrive\\Desktop\\RoadSight\\RoadSight\\ml\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# We import standard Python libraries\n",
    "# These help us interact with the operating system\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Print Python version to ensure reproducibility\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Print current working directory\n",
    "# This helps us confirm where the notebook is running\n",
    "print(\"Current directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7cf3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images:\n",
      "Japan_000000.jpg\n",
      "Japan_000001.jpg\n",
      "Japan_000002.jpg\n",
      "Japan_000003.jpg\n",
      "Japan_000004.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to one country's images\n",
    "image_dir = \"../data/raw/RDD2020/Japan/images\"\n",
    "\n",
    "# List first 5 image files\n",
    "images = os.listdir(image_dir)[:5]\n",
    "\n",
    "print(\"Sample images:\")\n",
    "for img in images:\n",
    "    print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d33f583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan_000000.jpg → 1\n",
      "Japan_000001.jpg → 1\n",
      "Japan_000002.jpg → 1\n",
      "Japan_000003.jpg → 1\n",
      "Japan_000004.jpg → 1\n",
      "Japan_000005.jpg → 1\n",
      "Japan_000006.jpg → 1\n",
      "Japan_000008.jpg → 1\n",
      "Japan_000009.jpg → 1\n",
      "Japan_000010.jpg → 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Path to images and annotations\n",
    "image_dir = \"../data/raw/RDD2020/Japan/images\"\n",
    "label_dir = \"../data/raw/RDD2020/Japan/annotations/xmls\"\n",
    "\n",
    "# Look at first 10 images (safe for testing)\n",
    "image_files = sorted(os.listdir(image_dir))[:10]\n",
    "\n",
    "for img in image_files:\n",
    "    # Convert image filename to corresponding XML filename\n",
    "    label_file = img.replace(\".jpg\", \".xml\")\n",
    "    label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "    # Default: assume no damage\n",
    "    label = 0\n",
    "\n",
    "    # Check if annotation file exists\n",
    "    if os.path.exists(label_path):\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Find all <object> tags (each represents road damage)\n",
    "        objects = root.findall(\"object\")\n",
    "\n",
    "        # If at least one object exists → damage present\n",
    "        if len(objects) > 0:\n",
    "            label = 1\n",
    "\n",
    "    print(img, \"→\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632018a3-dfc4-486c-99c7-5e259567d94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample annotation files:\n",
      "xmls\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "label_dir = \"../data/raw/RDD2020/Japan/annotations/xmls\"\n",
    "\n",
    "xml_files = os.listdir(label_dir)[:10]\n",
    "\n",
    "print(\"Sample annotation files:\")\n",
    "for f in xml_files:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 10506\n",
      "Example 0 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 1 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 2 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 3 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 4 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 5 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 6 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 7 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 8 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 9 → shape: torch.Size([3, 600, 600]), label: 0\n",
      "Example 10 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 11 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 12 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 13 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 14 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 15 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 16 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 17 → shape: torch.Size([3, 1080, 1080]), label: 1\n",
      "Example 18 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 19 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 20 → shape: torch.Size([3, 1024, 1024]), label: 1\n",
      "Example 21 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 22 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 23 → shape: torch.Size([3, 600, 600]), label: 1\n",
      "Example 24 → shape: torch.Size([3, 600, 600]), label: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from training.dataset import RoadSightDataset\n",
    "\n",
    "image_dir = \"../data/raw/RDD2020/Japan/images\"\n",
    "annotation_dir = \"../data/raw/RDD2020/Japan/annotations/xmls\"\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = RoadSightDataset(image_dir, annotation_dir)\n",
    "\n",
    "print(\"Total images:\", len(dataset))\n",
    "\n",
    "# Test first 5 examples\n",
    "for i in range(5):\n",
    "    img, label = dataset[i]\n",
    "    print(f\"Example {i} → shape: {img.shape}, label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e4b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from training.dataset import RoadSightDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19390a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image preprocessing steps\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),           # Resize all images to 224x224\n",
    "    transforms.ToTensor(),                    # Convert PIL image to tensor [C,H,W] in range [0,1]\n",
    "    transforms.Normalize(                      # Normalize using ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920857dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/raw/RDD2020/Japan/images\"\n",
    "annotation_dir = \"../data/raw/RDD2020/Japan/annotations/xmls\"\n",
    "\n",
    "dataset = RoadSightDataset(image_dir, annotation_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b672d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,      # Shuffle for training\n",
    "    num_workers=2      # Use 2 CPU threads for loading (adjust to your machine)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7707eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image tensor shape: torch.Size([32, 3, 224, 224])\n",
      "Batch labels: tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch image tensor shape:\", images.shape)\n",
    "print(\"Batch labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68ab9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from training.dataset import RoadSightDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b74d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5dee4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "image_dir = \"../data/raw/RDD2020/Japan/images\"\n",
    "annotation_dir = \"../data/raw/RDD2020/Japan/annotations/xmls\"\n",
    "\n",
    "dataset = RoadSightDataset(image_dir, annotation_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56e5c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saima\\OneDrive\\Desktop\\RoadSight\\RoadSight\\ml\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\saima\\OneDrive\\Desktop\\RoadSight\\RoadSight\\ml\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\saima/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac03b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c723dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: 0.3424\n",
      "Batch 20, Loss: 0.3402\n",
      "Batch 30, Loss: 0.2888\n",
      "Batch 40, Loss: 0.2836\n",
      "Batch 50, Loss: 0.2548\n",
      "Batch 60, Loss: 0.2595\n",
      "Batch 70, Loss: 0.2974\n",
      "Batch 80, Loss: 0.2165\n",
      "Batch 90, Loss: 0.3359\n",
      "Batch 100, Loss: 0.2114\n",
      "Batch 110, Loss: 0.3196\n",
      "Batch 120, Loss: 0.2043\n",
      "Batch 130, Loss: 0.2333\n",
      "Batch 140, Loss: 0.1959\n",
      "Batch 150, Loss: 0.3136\n",
      "Batch 160, Loss: 0.2553\n",
      "Batch 170, Loss: 0.2198\n",
      "Batch 180, Loss: 0.2496\n",
      "Batch 190, Loss: 0.2707\n",
      "Batch 200, Loss: 0.2548\n",
      "Batch 210, Loss: 0.2537\n",
      "Batch 220, Loss: 0.1949\n",
      "Batch 230, Loss: 0.1929\n",
      "Batch 240, Loss: 0.3068\n",
      "Batch 250, Loss: 0.2306\n",
      "Batch 260, Loss: 0.2933\n",
      "Batch 270, Loss: 0.2843\n",
      "Batch 280, Loss: 0.2569\n",
      "Batch 290, Loss: 0.2221\n",
      "Batch 300, Loss: 0.2467\n",
      "Batch 310, Loss: 0.2234\n",
      "Batch 320, Loss: 0.1865\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1  # Start small\n",
    "model.train()    # Set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Batch {i+1}, Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e7c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/roadsight_v1.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
